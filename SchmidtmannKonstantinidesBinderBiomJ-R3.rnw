\documentclass[bimj,fleqn]{w-art}
\usepackage{color}  % included for highlighting (04.11.2017 IS)
\usepackage{times}
\usepackage{w-thm}
\usepackage[authoryear]{natbib}
\setlength{\bibsep}{2pt}
\setlength{\bibhang}{2em}
\newcommand{\J}{J\"{o}reskog}
\newcommand{\So}{S\"{o}rbom}
\newcommand{\bcx}{{\bf X}}
\newcommand{\bcy}{{\bf Y}}
\newcommand{\bcz}{{\bf Z}}
\newcommand{\bcu}{{\bf U}}
\newcommand{\bcv}{{\bf V}}
\newcommand{\bcw}{{\bf W}}
\newcommand{\bci}{{\bf I}}
\newcommand{\bch}{{\bf H}}
\newcommand{\bcb}{{\bf B}}
\newcommand{\bcr}{{\bf R}}
\newcommand{\bcm}{{\bf M}}
\newcommand{\bcf}{{\bf F}}
\newcommand{\bcg}{{\bf G}}
\newcommand{\bcs}{{\bf S}}
\newcommand{\bca}{{\bf A}}
\newcommand{\bcd}{{\bf D}}
\newcommand{\bcc}{{\bf C}}
\newcommand{\bce}{{\bf E}}
\newcommand{\ba}{{\bf a}}
\newcommand{\bb}{{\bf b}}
\newcommand{\bc}{{\bf c}}
\newcommand{\bd}{{\bf d}}
\newcommand{\bx}{{\bf x}}
\newcommand{\by}{{\bf y}}
\newcommand{\bz}{{\bf z}}
\newcommand{\bu}{{\bf u}}
\newcommand{\bv}{{\bf v}}
\newcommand{\bh}{{\bf h}}
\newcommand{\bl}{{\bf l}}
\newcommand{\be}{{\bf e}}
\newcommand{\br}{{\bf r}}
\newcommand{\bw}{{\bf w}}
\newcommand{\de}{\stackrel{D}{=}}
\newcommand{\bt}{\bigtriangleup}
\newcommand{\bfequiv}{\mbox{\boldmath $\equiv$}}
\newcommand{\bmu}{\mbox{\boldmath $\mu$}}
\newcommand{\bnu}{\mbox{\boldmath $\nu$}}
\newcommand{\bxi}{\mbox{\boldmath $\xi$}}
\newcommand{\btau}{\mbox{\boldmath $\tau$}}
\newcommand{\bgamma}{\mbox{\boldmath $\Gamma$}}
\newcommand{\bphi}{\mbox{\boldmath $\Phi$}}
\newcommand{\bfphi}{\mbox{\boldmath $\varphi$}}
\newcommand{\bfeta}{\mbox{\boldmath $\eta$}}
\newcommand{\bpi}{\mbox{\boldmath $\Pi$}}
\newcommand{\bequiv}{\mbox{\boldmath $\equiv$}}
\newcommand{\bvarepsilon}{\mbox{\boldmath $\varepsilon$}}
\newcommand{\btriangle}{\mbox{\boldmath $\triangle$}}
\newcommand{\bdelta}{\mbox{\boldmath $\Delta$}}
\newcommand{\beps}{\mbox{\boldmath $\epsilon$}}
\newcommand{\btheta}{\mbox{\boldmath $\theta$}}
\newcommand{\balpha}{\mbox{\boldmath $\alpha$}}
\newcommand{\bsphi}{\mbox{\boldmath $\varphi$}}
\newcommand{\bsig}{\mbox{\boldmath $\sigma$}}
\newcommand{\bfpsi}{\mbox{\boldmath $\psi$}}
\newcommand{\bfdelta}{\mbox{\boldmath $\delta$}}
\newcommand{\bsigma}{{\bf \Sigma}}
\newcommand{\bzero}{{\bf 0}}
\newcommand{\bpsi}{\mbox{\boldmath $\Psi$}}
\newcommand{\bep}{\mbox{\boldmath $\epsilon$}}
\newcommand{\bomega}{\mbox{\boldmath $\Omega$}}
\newcommand{\bfomega}{\mbox{\boldmath $\omega$}}
\newcommand{\blambda}{\mbox{\boldmath $\Lambda$}}
\newcommand{\bflambda}{\mbox{\boldmath $\lambda$}}
\newcommand{\bfsigma}{\mbox{\boldmath $\sigma$}}
\newcommand{\bfpi}{{\mbox{\boldmath $\pi$}}}
\newcommand{\bupsilon}{\mbox{\boldmath $\upsilon$}}
\newcommand{\obs}{{\rm obs}}
\newcommand{\mis}{{\rm mis}}
\theoremstyle{plain}
\newtheorem{criterion}{Criterion}
\theoremstyle{definition}
\newtheorem{condition}[theorem]{Condition}
\usepackage[]{graphicx}
\chardef\bslash=`\\ % p. 424, TeXbook
\newcommand{\ntt}{\normalfont\ttfamily}
\newcommand{\cn}[1]{{\protect\ntt\bslash#1}}
\newcommand{\pkg}[1]{{\protect\ntt#1}}
\let\fn\pkg
\let\env\pkg
\let\opt\pkg
\hfuzz1pc % Don't bother to report overfull boxes if overage is < 1pc
\newcommand{\envert}[1]{\left\lvert#1\right\rvert}
\let\abs=\envert

\begin{document}
% 3rd revision of paper submitted to Biometrical Journal

% initial chunk that loads necessary libraries
<<init, include=FALSE>>=
# load all libraries and functions
library(knitr)
library(xtable)
library(latex2exp)
library(flexsurv)

# obtain session information
sessionInfo.txt <- sessionInfo()

# set global chunk options
opts_chunk$set(warning = FALSE, message = FALSE)
@


<<define_functions, echo=FALSE, eval=TRUE, cache = TRUE>>=
# This chunk defines the functions needed here
# ------------------------------------------------------------------------------
# function moments computes mean and variance of Wilcoxon-Mann-Whitney statistic
# in the presence of death-censored observations
# ------------------------------------------------------------------------------
# Assumptions:
# - normal one-sided test for difference $\epsilon = 0 $
# - non-inferiority margin $\epsilon > 0$

# it returns $\mu_u$ and $\sigma_u$

# function parameters
# $p_0 = $ probability of censoring by death in reference group
# $p_1 = $ probability of censoring by death in treatment group
# $\mu_0 = $, mean in reference group
# $\mu_1 = $, mean in treatment group
# $\sigma =  SD(X_i), i=0, 1$
# $\tau$ = time when quantitative endpoint is determined, this is needed to
# calculate the hazards
# $n_0 = $ sample size in reference group
# $n_1 = $ sample size in treatment group
# verbose = TRUE / FALSE controls output
# tied = TRUE / FALSE determines whether untied or tied version should be computed
# ------------------------------------------------------------------------------

moments <- function(p_0 = 0.2, p_1 = 0.3, mu_0 = 1, mu_1 = 1, sigma = 1, tau = 1,
                    n_0 = 50, n_1 = 50, verbose = FALSE, tied = FALSE){
  q_0 <- 1 - p_0
  q_1 <- 1 - p_1
  # pi_t1, pi_t2 and pi_t3 are only needed in the untied case
  if (!tied) {
    # Non-null mortality risk in both groups
    if (p_0 > 0 && p_1 > 0) {
      lambda_0 <- -log(q_0) / tau
      lambda_1 <- -log(q_1) / tau
      HR <- lambda_1 / lambda_0
      pi_t1 <- (p_0 * (p_1  - 1) * HR / (1 + HR) + p_1 / (1 + HR)) / (p_0 * p_1)
      pi_t2 <- (p_1 - 2 * HR / (1 + HR) * (1 - q_0 * q_1) + (1 - q_0^2 * q_1) * HR / (2 + HR) ) / (p_0^2 * p_1)
      pi_t3 <- (p_0 * q_1^2 + 2 * q_1 * (q_0 * q_1 - 1) / (1 + HR) - (q_0 * q_1^2 - 1) / (1 + 2*HR)) / (p_0 * p_1^2)
      if (verbose) {
        print(paste0("HR = ", HR))
        print(paste0("pi_t1 = ", pi_t1))
        print(paste0("pi_t2 = ", pi_t2))
        print(paste0("pi_t3 = ", pi_t3))
      }
    } else{
      # if there is no mortality risk in at least one group then the terms with
      # pi_t1, pi_t2, and pi_t3 vanish, i. e.
      pi_t1 <- 0
      pi_t2 <- 0
      pi_t3 <- 0
    }
  }
  delta <- mu_1 - mu_0
  shift <- delta / sigma
  if (verbose) {
    print(paste0("delta = ", delta))
    print(paste0("shift = ", shift))
  }
  # integrand1, integrand2, integrand3 define functions that have to be
  # intergrated in or der obtain pi_x1, pi_x2, pi_x3 if the quantitative
  # endpoint has a normal distribution
  integrand1 <- function(x) {
    return(dnorm(x - shift)*pnorm(x))
  }
  integrand2 <- function(x) {
    return(dnorm(x - shift)*(pnorm(x))^2)
  }

  integrand3 <- function(x) {
    return(dnorm(x)*(pnorm(x - shift))^2)
  }

  pi_x1 <- integrate(integrand1, -Inf, Inf)$value
  pi_x2 <- integrate(integrand2, -Inf, Inf)$value
  pi_x3 <- 2*pi_x1 - 1 + integrate(integrand3, -Inf, Inf)$value

  if (verbose) {
    print(paste0("pi_x1 = ", pi_x1))
    print(paste0("pi_x2 = ", pi_x2))
    print(paste0("pi_x3 = ", pi_x3))
  }
  if (!tied) {
    pi_u1 <- p_0 * p_1 * pi_t1 + p_0 * q_1 + q_0 * q_1 * pi_x1
    pi_u2 <- p_0^2 * q_1 + p_0^2 * p_1 * pi_t2 + 2 * p_0 * q_0 * q_1 * pi_x1 + q_0^2 * q_1 * pi_x2
    pi_u3 <- p_0 * q_1^2 + p_0 * p_1^2 * pi_t3 + 2 * p_0 * p_1 * q_1 * pi_t1 + q_0 * q_1^2 * pi_x3
  }else{
    pi_u1 <- p_0 * p_1 / 2 + p_0 * q_1 + q_0 * q_1 * pi_x1
    pi_u2 <- p_0^2 * q_1 + p_0^2 * p_1 / 3 + 2 * p_0 * q_0 * q_1 * pi_x1 + q_0^2 * q_1 * pi_x2
    pi_u3 <- p_0 * q_1^2 + p_0 * p_1^2 / 3 + p_0 * p_1 * q_1  + q_0 * q_1^2 * pi_x3
  }
  if (verbose) {
    print(paste0("pi_u1 = ", pi_u1))
    print(paste0("pi_u2 = ", pi_u2))
    print(paste0("pi_u3 = ", pi_u3))
  }

  mu_u <- pi_u1
  if (!tied) {
    sigma_u <- sqrt((pi_u1 * (1 - pi_u1)
                     + (n_0 - 1)*(pi_u2 - pi_u1^2)
                     + (n_1 - 1)*(pi_u3 - pi_u1^2)) / (n_0 * n_1) )
  }else{
    sigma_u <- sqrt((pi_u1 * (1 - pi_u1)
                     + (n_0 - 1)*(pi_u2 - pi_u1^2 - p_0^2 * p_1 / 12)
                     + (n_1 - 1)*(pi_u3 - pi_u1^2 - p_0 * p_1^2 / 12) - p_0 * p_1 / 4) / (n_0 * n_1) )
  }
  if (verbose) {
    print(paste0("mu_u = ", mu_u))
    print(paste0("sigma_u = ", sigma_u))
  }
  list(mu_u, sigma_u)
  return(list(mu_u = mu_u, sigma_u = sigma_u))
}

# ------------------------------------------------------------------------------
# Function power_gr determines power for the Wilcoxon-Mann-Whitney test for
# non-inferiority in the presence of death-censored observations
# ------------------------------------------------------------------------------
# Assumptions
# - normal one-sided test for difference $\epsilon = 0$
# - non-inferiority margin $\epsilon > 0$

# epsilon is computed from \mu_0$ under $H_0$
# $H_0: \mu(U) = P(\tilde{X_{0k}} < \tilde{X_{1l}}) \leq \frac{1}{2} - \epsilon$
# $H_1: \mu(U) = P(\tilde{X_{0k}} < \tilde{X_{1l}}) > \frac{1}{2} - \epsilon $

# Function parameters.
# $p_0_0 = p_0$ under $H_0$ (probability of censoring by death in reference group under $H_0$)
# $p_1_0 = p_1$ under $H_0$ (probability of censoring by death in treatment group under $H_0$)
# $p_0_1 = p_0$ under $H_1$ (probability of censoring by death in reference group under $H_1$)
# $p_1_1 = p_1$ under $H_1$ (probability of censoring by death in treatment group under $H_1$)
# $\mu_0_0 = \mu_0$ under $H_0$ (mean in reference group under $H_0$)
# $\mu_1_0 = \mu_1$ under $H_0$ (mean in treatment group under $H_0$)
# $\mu_0_1 = \mu_0$ under $H_1$ (mean in reference group under $H_1$)
# $\mu_1_1 = \mu_1$ under $H_1$ (mean in treatment group under $H_1$)
# $\sigma =  var(X_i), i=0, 1$ under $H_0$ and $H_1 $
# $\tau$ = time when quantitative endpoint is determined, this is needed to calculate
# the hazards
# $n_0 = $ sample size in reference group
# $n_1 = $ sample size in treatment group
# verbose = TRUE / FALSE controls output
# tied = TRUE / FALSE determines whether untied or tied version should be computed

power_gr <- function(alpha,
                     p_0_0 = 0.2, p_1_0 = 0.2, p_0_1 = 0.2, p_1_1 = 0.2,
                     mu_0_0 = 1, mu_1_0 = 1, mu_0_1 = 1, mu_1_1 = 1,
                     sigma = 1, tau = 1,
                     n_0 = 50, n_1 = 50, verbose = FALSE, tied = FALSE){
  # determine moments unter $H_0$
  moments_H_0 <- moments(p_0 = p_0_0, p_1 = p_1_0, mu_0 = mu_0_0, mu_1 = mu_1_0,
                         sigma = sigma, tau = tau, n_0 = n_0, n_1 = n_1,
                         verbose = verbose, tied = tied)
  # compute effective epsilon, given parameters under $H_0$
  epsilon <- 0.5 - moments_H_0$mu_u

  if (verbose) {
    print("moments unter H_0")
    print(paste0("mu_u_0 = ", moments_H_0$mu_u))
    print(paste0("sigma_u_0 = ", moments_H_0$sigma_u))
  }
  # determine moments unter $H_1$
  moments_H_1 <- moments(p_0 = p_0_1, p_1 = p_1_1, mu_0 = mu_0_1, mu_1 = mu_1_1,
                         sigma = sigma, tau = tau, n_0 = n_0, n_1 = n_1,
                         verbose = verbose)
  if (verbose) {
    print("moments unter H_1")
    print(paste0("mu_u_1 = ", moments_H_1$mu_u))
    print(paste0("sigma_u_1 = ", moments_H_1$sigma_u))
  }
  # compute $1 - \alpha$ percentile of the standard normal distribution
  z_alpha <- qnorm(alpha, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)
  power = pnorm((z_alpha * moments_H_0$sigma_u
                 + moments_H_1$mu_u - moments_H_0$mu_u) / moments_H_1$sigma_u)
  list(power, epsilon)
  return(list(power = power, epsilon = epsilon))
}

# ------------------------------------------------------------------------------
# Function generate_dco generates death censored observations for two groups.
#
# The time to death follows a log-logistic function with scale parameters
# $a_0, a_1$ and shape parameters $b_0, b_1$ respectively. Scale parameters are
# determined based on probability of death until time $\tau$
# The quantitative parameter follows a normal distribution with means
# $\mu_0, mu_1$ respectively, and commmon variance $\sigma^2$
#
# Time to death and the quantitative endpoint are here assumed to be independent
# ------------------------------------------------------------------------------

# it returns a list vectors consisting of
# $x_0$ uncensored values for the quantitative variable in the reference group
# $x_1$ uncensored values for the quantitative variable in the treatment group
# $t_0$ time to death in the reference group
# $t_1$ time to death in the treatment group
# $xx_0 = x_0$ for uncensored observations,
#              $min(x_0, x_1) - 1 + t_0$ for the censored observations in the
#              reference group in the untied case
#              $min(x_0, x_1) - 1$ for the censored observations in the
#              reference group in the tied case
# $xx_1 = x_1$ for uncensored observations,
#              $min(x_0, x_1) - 1 + t_1$ for the censored observations in the
#              reference group in the untied case
#              $min(x_0, x_1) - 1$ for the censored observations in the
#              reference group in the tied case


# function parameters
# $n_0 = $ sample size in reference group
# $n_1 = $ sample size in treatment group
# $p_0 = $ probability of censoring by death in reference group
# $p_1 = $ probability of censoring by death in treatment group
# $\mu_0 = $, mean in reference group
# $\mu_1 = $, mean in treatment group
# $\sigma =  SD(X_i), i=0, 1$
# $\tau$ = time when quantitative endpoint is determined, this is needed to
# obtain the scale parameters of the log-logistic distribution
# $b_0 = $ shape parameter in reference group
# $b_1 = $ shape parameter in treatment group
# verbose = TRUE / FALSE controls output
# tied = TRUE / FALSE determines whether untied or tied version should be
# computed

generate_dco <- function(n_0 = 50, n_1 = 50,
                         p_0 = 0.2, p_1 = 0.2,
                         mu_0 = 1, mu_1 = 1, sigma = 1, tau = 1,
                         b_0 = 1, b_1 = 1,
                         verbose = FALSE, tied = FALSE){
# $a_0 = $ scale parameter in reference group, derived from p_0
# $a_1 = $ scale parameter in treatment group, derived from p_1
  q_0 <- 1 - p_0
  q_1 <- 1 - p_1
  t_0 <- rep(NA, n_0)
  t_1 <- rep(NA, n_1)
  if (p_0 > 0) {
    # determine scale parameter
    a_0 <- tau * (q_0 / p_0)^(1/b_0)
    # generate time to death variable
    t_0 <- rllogis(n_0, shape = b_0, scale = a_0)
    # generate death indicators
    d_0 <- ifelse(t_0 <= tau, 1, 0)
  }else{
    d_0 <- rep(0, n_0)
  }
  if (p_1 > 0) {
    # determine scale parameter
    a_1 <- tau * (q_1 / p_1)^(1/b_1)
    # generate time to death variable
    t_1 <- rllogis(n_1, shape = b_1, scale = a_1)
    # generate death indicators
    d_1 <- ifelse(t_1 <= tau, 1, 0)
  }else{
    d_1 <- rep(0, n_1)
  }
  # generate quantiative endpoints
  x_0 <- rnorm(n_0, mean = mu_0, sd = sigma)
  x_1 <- rnorm(n_1, mean = mu_1, sd = sigma)
  eta <- min(c(x_0, x_1)) - 1 - tau
  if (!tied) {
    xx_0 <- ifelse(d_0 == 1, eta + t_0, x_0)
    xx_1 <- ifelse(d_1 == 1, eta + t_1, x_1)
  }else {
    xx_0 <- ifelse(d_0 == 1, eta, x_0)
    xx_1 <- ifelse(d_1 == 1, eta, x_1)
  }
  if (verbose) {
    print(paste0("t_0 = ", round(t_0, digits = 3)))
    print(paste0("t_1 = ", round(t_1, digits = 3)))
    print(paste0("d_0 = ", round(d_0, digits = 3)))
    print(paste0("d_1 = ", round(d_1, digits = 3)))
    print(paste0("x_0 = ", round(x_0, digits = 3)))
    print(paste0("x_1 = ", round(x_1, digits = 3)))
    print(paste0("eta = ", round(eta, digits = 3)))
    print(paste0("xx_0 = ", round(xx_0, digits = 3)))
    print(paste0("xx_1 = ", round(xx_1, digits = 3)))
  }
  list(xx_0, xx_1, x_0, x_1, t_0, t_1)
  return(list(xx_0 = xx_0, xx_1 = xx_1,
              x_0 = x_0, x_1 = x_1,
              t_0 = t_0, t_1 = t_1))
}

# set global chunk options
opts_chunk$set(warning = FALSE, message = FALSE)
@
%\DOIsuffix{bimj.DOIsuffix}
\DOIsuffix{bimj.200100000}
\Volume{52}
\Issue{61}
\Year{2010}
\pagespan{1}{}
\keywords{Censoring by death; Global rank test; Non-inferiority;
Combined endpoints; Pulmonary embolism;\\
\noindent\hspace*{-4.2pc} Supporting Information for this article is available
on the WWW under
\underline{https://github.com/IreneSchmidtmann/GlobalRankTest}. Source code to
reproduce the results is available as supporting information on the journal's
web page \underline{http://onlinelibrary.wiley.com/doi/xxx/
suppinfo}.
}  %%% semicolon and fullpoint added here for keyword style

\title[Non-inferiority test death censored]{Power of the Wilcoxon-Mann-Whitney
test for non-inferiority in the presence of death-censored observations}
%% Information for the first author.
\author[Schmidtmann]{Irene Schmidtmann\footnote{Corresponding author: {\sf{e-mail: Irene.Schmidtmann@uni-mainz.de}}, Phone: +49-6131-173951, Fax: +49-6131-172968}\inst{,1}}
\address[\inst{1}]{Institute for Medical Biostatistics, Epidemiology and Informatics (IMBEI),
University Medical Center Johannes Gutenberg University Mainz, D 55101 Mainz}
%%%%    Information for the second author
\author[Konstantinides]{Stavros Konstantinides\inst{2}}
\address[\inst{2}]{Center for Thrombosis and Hemostasis (CTH), University Medical Center Johannes Gutenberg University Mainz, D 55101 Mainz}
%%%%    Information for the third author
\author[Binder]{Harald Binder\inst{3}}
\address[\inst{3}]{Institute of Medical Biometry and Statistics, Medical Faculty and Medical Center -- University Freiburg, Stefan-Meier-Str. 26, D 79104 Freiburg}
%%%%    \dedicatory{This is a dedicatory.}
\Receiveddate{zzz} \Reviseddate{zzz} \Accepteddate{zzz}

\begin{abstract}
In clinical trials with patients in a critical state, death may preclude
measurement of a quantitative endpoint of interest, and even early measurements,
e.g. for intention-to-treat analysis, may not be available. For example, a
non-negligible proportion of patients with acute pulmonary embolism will die
before 30 day measurements on the efficacy of thrombolysis can be obtained. As
excluding such patients may introduce bias, alternative analyses and
corresponding means for sample size calculation are needed. We specifically
consider power analysis in a randomized clinical trial setting in which the goal
is to demonstrate non-inferiority of a new treatment as compared to a reference
treatment. Also, a non-parametric approach may be needed due to the distribution of
the quantitative endpoint of interest. While some approaches have been developed
in a composite endpoint setting, our focus is on the continuous endpoint affected
by death-related censoring, for which no approach for non-inferiority is
available. We propose a solution based on ranking the quantitative outcome and
assigning “worst rank” scores to the patients without quantitative outcome
because of death. Based on this, we derive power formulae for a non-inferiority
test in the presence of death-censored observations, considering settings with
and without ties. The approach is illustrated for an examplary clinical trial in
pulmonary embolism. The results there show a substantial effect of death on
power, also depending on differential effects in the two trial arms. Therefore,
use of the proposed formulae is advisable whenever there is death to be expected
before measurement of a quantitative primary outcome of interest.
\end{abstract}



%% maketitle must follow the abstract.
\maketitle                   % Produces the title.

\section{Introduction}
\label{sec:Intro}

In clinical trials for cardiovascular diseases, often the aim of an
intervention is to improve functional capacity, measured by some quantitative
surrogate endpoint such as the six minute walking distance, biomarker levels,
or echocardiographic parameters of cardiovascular function.

Typically, unfavourable outcomes of the quantitative endpoint are related to a
higher risk of cardiovascular death and consequently there is a non-negligible
probability of an early fatal outcome. Thus, censoring by
death may occur if a patient dies before the quantitative outcome can be
determined. Censoring by death leads to missing values which are most unlikely
to be missing at random. Therefore, ignoring this kind of missing values and
excluding them from analysis does not only decrease power but may also lead to
biased estimates of treatment effect. It also contradicts the
intention-to-treat principle when not all patients included in the trial are
included in the analysis.

Worst-rank scores have been considered e. g. by Wittes et al (1989) and Lachin
(1999) to tackle this problem. Without loss of generality we assume that
higher values of the quantitative outcome are more favourable than lower
values. We further assume that any quantitative outcome is better than death
and that earlier death is worse than later death. This leads to an obvious
ordering of outcomes which motivates the definition of worst-rank scores.

Tied worst-rank scores are obtained by allocating a rank score corresponding
to a single value below the minimum observed value of the quantitative
endpoint. Untied worst-rank scores can be obtained, if the time of death is
taken into account. The lowest rank score is then allocated to the patient
that has died first, subsequent deceased individuals receive ranks according
to their time of death. After the deceased have been ranked, the subsequent
ranks are allocated to the surviving patients according to their quantitative
outcome.

Lachin (1999) has shown that such an approach is unbiased against a restricted
one-sided alternative which states that the new treatment is better with
respect to both, mortality and the quantitative endpoint or at least better
with respect to one criterion and equal with respect to the other.

Another view of this approach is to consider it as a global ranking of
multiple endpoints, e. g. some kind of event - such as death - and some
quantitative measurement - such as a biomarker. This approach was originally
suggested by O'Brien (1984) for multiple endpoints and applied in cardiology
e. g. by Felker at al (2008) and Felker and Maisel (2010) combining several
binary and quantitative endpoints.

We here consider the situation where a new treatment is compared to a standard
treatment and the intention is to show that the new treatment is non-inferior
to the standard treatment when comparing a quantitative measurement that may
be censored by death. As above this corresponds to a one-sided hypothesis.
The alternative hypothesis in this case states that the new treatment is
non-inferior to the standard treatment with respect to both the quantitative
endpoint and the mortality risk.

A Mann-Whitney test for equivalence is e. g. described by Wellek (2010); the
non-inferiority test is the special case in which one equivalence boundary is set
to infinity. Wellek provides mean and standard deviation under the (equivalence)
null hypothesis.

Matsouaka and Bentensky (2015) have derived power and sample size formulae for
the one-sided test of the null hypothesis of equality against a restricted
alternative as described by Lachin (1999). They present specific formulae for
common distributions of quantitative endpoint and time to event, amongst them
a normally distributed quantitative endpoint and exponentially distributed
time to event. We extend this approach to encompass a more general null
hypothesis.

However, to the best of our knowledge, so far no power or sample size formula
for the Wilcoxon-Mann-Whitney test for non-inferiority in the presence of
death-censored observations is available.

In section ~\ref{sec:ThrombolysisApplication}, we present the
application, a clinical trial in pulmonary embolism, that motivated our
methodological work. After setting out the notation in section
~\ref{sec:Notation}, the power formulae are derived in section ~\ref{sec:Power}.
Section ~\ref{sec:Application} illustrates the application of the derived
formulae to the planned clinical trial. The validity of the derived formula
is explored in a small simulation study. A discussion and concluding remarks
are provided in section ~\ref{sec:Discussion}.


\section{Thrombolysis application}
\label{sec:ThrombolysisApplication}
The methodological considerations presented in this paper were motivated by
planning a clinical trial studying treatment of patients with
intermediate-high-risk pulmonary embolism. These are patients who are
normotensive and appear hemodynamically stable at presentation, but have
evidence of right ventricular dysfunction on echocardiography in addition to
elevated cardiac troponin levels in the circulation.

In this trial, the standard would be to administer standard-dose systemic
thrombolytic treatment. The new experimental treatment would be catheter-directed,
ultrasound-assisted low dose thrombolysis. The right to left ventricular (RV/LV)
diameter ratio on echocardiography 24 hours after the intervention was
considered a suitable primary endpoint. Unfortunately, fatal
outcomes do occur within hours after treatment in patients with this condition.
This precludes the observation of the RV/LV diameter ratio 24 hours after the
intervention. Therefore, taking into account censoring by death of the
quantitative endpoint was necessary.

While little difference in the primary endpoint and mortality was expected,
the potential benefit of the low dose thrombolysis would be lower bleeding
risk. Therefore, as primary endpoint analysis, we chose to test for
non-inferiority and to apply a Wilcoxon-Mann-Whitney test for non-inferiority
in the presence of death-censored observations.

\section{Notation and hypotheses}
\label{sec:Notation}
Let  $\tau (\tau > 0)$ denote the time from treatment to planned determination
of the quantitative endpoint. This is intended to be identical for all patients.
All patients are observed at least until time $\tau$ unless they die before this
time. If patients die before time $\tau$ it is assumed that the time of death
can be determined sufficiently precisely. Let further $X_{01}, \ldots, X_{0n_0}$
denote the values of the quantitative endpoint in the $n_0$ indiviudals in the
reference group with standard treatment ($i=0$) and $X_{11}, \ldots, X_{1n_1}$
the values of the quantitative endpoint in the $n_1$ indiviudals in the group
with the new experimental treatment ($i=1$). The individual event times in group
$i$ are given by $T_{i1}, \ldots, T_{in_i}$. They may not be observed if patients
are alive at time $\tau$. Let $D_{ik} = 1$ indicate that subject $k$ in group
$i$ dies before the quantitative endpoint $X_{ik}$ can be determined, i. e.
$D_{ik} = 1$ if $T_{ik} < \tau $, $D_{ik} = 0$ otherwise. The survival
probability in group $i$ up to time $\tau$ is given by $q_i = S_i(\tau)$, where
$S_{i}(t)$ is the survival function. Accordingly, the cumulative mortality in
group $i$ up to time $\tau$ is given by $p_i = 1 - q_i = \text{P}(D_{ik} = 1)$.

Here, without loss of generality, we assume that high values of $X_{ik}$ are
favourable. Otherwise we could consider $-X_{ik}$. We further assume that
death is worse than any quantitative outcome and -- in the untied worst rank
case -- that early death is worse than later death.

Following Matsouaka and Betensky (2015), we introduce a new variable, on which
ranking can be based. This variable is constructed such that all patients who
die before time $\tau$ have lower values than the patients who survive past
$\tau$ and are ranked according to their survival times. Patients who survive
past time $\tau$ are ranked according to their observed value of the
quantitative endpoint. The new variable is defined as
$\widetilde{X}_{ik} = D_{ik}(\eta  + T_{ik}) + (1 - D_{ik})X_{ik}$ with
$\eta = \min(X_{01}, \ldots, X_{0n_0}, X_{11}, \ldots, X_{1n_1}) - 1 - \tau$
in the untied case. In the tied case all patients who die before the
quantitative endpoint can be determined have the same rank. Hence, a different
definition of $\widetilde{X}_{ik}$ is needed and we set
$\widetilde{X}_{ik} = D_{ik}\zeta   + (1 - D_{ik})X_{ik}$ with
$\zeta = \min(X_{01}, \ldots, X_{0n_0}, X_{11}, \ldots, X_{1n_1}) - 1$.
In both cases, the minimum is taken over the observed $X_{ik}$. Note, that
$\widetilde{X}_{ik} = {X}_{ik}$ holds for patients who survive past $\tau$ in
both cases.

The non-inferiority hypotheses are
\begin{align*}
  H_0 &:  \text{P}(\widetilde{X}_{0k} < \widetilde{X}_{1l})
          \leq \frac{1}{2} - \varepsilon \\
H_1 &:  \text{P}(\widetilde{X}_{0k} < \widetilde{X}_{1l})
          > \frac{1}{2} - \varepsilon
\end{align*}

with non-inferiority margin $\varepsilon > 0$ for any pair $(k, l)$ of
observations from the two treatment groups. As $\varepsilon > 0 $ is arbitrary
in our approach, this is an extention of the suggestion by Matsouaka and
Betensky in which $\varepsilon = 0 $.

\section{Power calculation}
\label{sec:Power}
\subsection{Untied case}
\label{sec:PowerUntied}
The $\widetilde{X}_{ik}$ as defined in section~\ref{sec:Notation} are used to
determine ranks and to compute the Wilcoxon-Mann-Whitney test statistic
$ U =(n_0 n_1)^{-1}(\sum_{k=1}^{n_0}
    \sum_{l=1}^{n_1}I(\widetilde{X}_{0k} < \widetilde{X}_{1l})) $.

In order to evaluate the power of the Wilcoxon-Mann-Whitney test, we use the
standardised version of the Wilcoxon-Mann-Whitney U statistic, i. e. the test
statistic is given by $U^* =(U - \mu_0(U)) / \sigma_0(U)$, where $\mu_0(U)$ is
the expectation and $\sigma^2_0(U)$ the variance of $U$ under the null
hypothesis. Then under the null hypothesis $U^*$ asymptotically follows a
standard normal distribution. The null hypothesis is rejected at level $\alpha$
if $U^* > z_{1 - \alpha}$ where $z_{1-\alpha} = \Phi^{-1}({1-\alpha})$ is the
$(1 - \alpha)$-percentile of the normal distribution.

Therefore we need the expectation $\mu(U)$ and variance $\sigma^2(U)$ of $U$.
They were derived by Matsouaka and Betensky (2015) and are as follows:
\begin{align*}
\mu(U) &= \text{P}(\widetilde{X}_{0k} < \widetilde{X}_{1l}) \mbox{ for arbitrary }
               \widetilde{X}_{0k}, \widetilde{X}_{1l} \\
       &= p_0 p_1 \pi_{t1} + p_0 q_1 + q_0 q_1 \pi_{x1} \\
       &= \pi_{U1}
\intertext{with}
\pi_{X1} &= \text{P}(X_{0k} < X_{1l}) \\
\pi_{T1} &= \text{P}(T_{0k} < T_{1l} | D_{0k} = D_{1l} = 1) \\
\sigma^2(U) &= (n_0 n_1)^{-1} \left\{ \pi_{U1} (1 - \pi_{U1}) +
                                  (n_0 - 1) (\pi_{U2} - \pi_{U1}^2) +
                                  (n_1 - 1) (\pi_{U3} - \pi_{U1}^2) \right\}
\intertext{with}
\pi_{U2} &= \text{P}(\widetilde{X}_{0k} < \widetilde{X}_{1l}, \widetilde{X}_{0k'} < \widetilde{X}_{1l}) \\
         &= p_0^2 q_1 + p_0^2 p_1 \pi_{t2} + 2 p_0 q_0 q_1 \pi_{x1} + q_0^2 q_1 \pi_{x2} \\
\pi_{X2} &= \text{P}(X_{0k} < X_{1l}, X_{0k'} < X_{1l}) \\
\pi_{T2} &= \text{P}(T_{0k} < T_{1l}, T_{0k'} < T_{1l} | D_{0k} = D_{0k'} = D_{1l} = 1) \\
\pi_{U3} &= \text{P}(\widetilde{X}_{0k} < \widetilde{X}_{1l}, \widetilde{X}_{0k} < \widetilde{X}_{1l'}) \\
         &= p_0 q_1^2 + p_0 p_1^2 \pi_{t3} + 2 p_0 p_1 q_1 \pi_{t1} + q_0 q_1^2 \pi_{x3} \\
\pi_{X3} &= \text{P}(X_{0k} < X_{1l}, X_{0k} < X_{1l'}) \\
\pi_{T3} &= \text{P}(T_{0k} < T_{1l}, T_{0k} < T_{1l'} | D_{0k} = D_{1l} = D_{1l'} =1)\\
         &{\hphantom{=}}  \mbox{ for arbitrary } \widetilde{X}_{0k}, \widetilde{X}_{0k'}, \widetilde{X}_{1l},
                                     \widetilde{X}_{1l'}, k \ne k', l \ne l'
\end{align*}

If assumptions about the distribution of the quantitative endpoint and event time
are made, $\mu(U)$ and $\sigma(U)$ can be computed. When determining the power,
we consider the boundary of the null hypothesis, i. e.
$\mu_0(U) = \text{P}(\widetilde{X}_{0k} < \widetilde{X}_{1l}) = 1/2 - \varepsilon$.
The power of the test for a specific alternative, where $\mu(U) = \mu_1(U)$
and $\sigma(U) = \sigma_1(U)$, is given by
\begin{align*}
  1 - \beta &= \text{P} \left(\frac{U - \mu_0(U)}{\sigma_0(U)}  >
                                 z_{1-\alpha} \mid H_1 \right) \\
            &= \text{P} \left(\frac{U - \mu_1(U)}{\sigma_1(U)}  >
                                 z_{1-\alpha}\frac{\sigma_0(U)}{\sigma_1(U)} +
                                 \frac{\mu_0(U) - \mu_1(U)}{\sigma_1(U)} \right) \\
            &= \Phi\left(- z_{1-\alpha}\frac{\sigma_0(U)}{\sigma_1(U)} -
                            \frac{\mu_0(U) - \mu_1(U)}{\sigma_1(U)} \right) \\
            &= \Phi\left(z_{\alpha}\frac{\sigma_0(U)}{\sigma_1(U)} +
                            \frac{\mu_1(U) - \mu_0(U)}{\sigma_1(U)} \right) ,
\end{align*}
where $\Phi$ denotes the distribution function of the standard normal
distribution.

If the specific alternative considered is equivalence of treatments, i. e.
$\widetilde{X}_{0k}$ and $\widetilde{X}_{1l}$ are from the same distribution,
we have $\mu_1(U) = \text{P}(\widetilde{X}_{0k} < \widetilde{X}_{1l}) = 1/2$
and $\sigma^2_1(U) = (n_0 + n_1 +1) /12 n_0 n_1$. The power is
then given by
\begin{align*}
1 - \beta &= \Phi\left(z_{\alpha}\frac{\sigma_0(U)}{\sigma_1(U)} +
                            \frac{\varepsilon}{\sigma_1(U)} \right) \\
          &= \Phi\left(z_{\alpha} \sqrt{\frac{12 \big\{ \pi_{U1} (1 - \pi_{U1}) +
                                  (n_0 - 1) (\pi_{U2} - \pi_{U1}^2) +
                                  (n_1 - 1) (\pi_{U3} - \pi_{U1}^2) \big\}}{n_0 + n_1 +1}} \right.\\
          &{\hphantom{= \Phi\bigg(}} \left. + \varepsilon \sqrt{\frac{12 n_0 n_1 }{n_0 + n_1 +1}} \right) .
\end{align*}

\subsection{Tied case}
\label{sec:PowerTied}
In the tied case we use the modified Wilcoxon-Mann-Whitney statistic $ \widetilde{U}$
which allows for ties and is defined as
$ \widetilde{U} =(n_0 n_1)^{-1}\sum_{k=1}^{n_0}
  \sum_{l=1}^{n_1}\left\{I(\widetilde{X}_{0k} < \widetilde{X}_{1l})
  + I(\widetilde{X}_{0k} = \widetilde{X}_{1l}) / 2 \right\} $.
As before, for determining the power the standardised version of the test
statistic is used:
$\widetilde{U}^* = (\widetilde{U} - \mu_0(\widetilde{U} )) / \sigma_0(\widetilde{U}) $
where
$\mu_0(\widetilde{U})$ is the expectation and $\sigma^2_0(\widetilde{U})$ the
variance of $\widetilde{U}$ under the null hypothesis. Under the null hypothesis
$\widetilde{U}^*$ also follows asymptotically a standard normal distribution.
Again, the null hypothesis is rejected at level $\alpha$ if
$\widetilde{U}^* > z_{1-\alpha}.$

Matsouaka and Betensky (2015) also derived expectation and variance in the tied
case which are as follows:

\begin{align*}
  \mu(\widetilde{U}) &= \text{P}(\widetilde{X}_{0k} < \widetilde{X}_{1l})
                        + \text{P}(\widetilde{X}_{0k} = \widetilde{X}_{1l}) / 2
                        \mbox{ for arbitrary }
                         \widetilde{X}_{0k}, \widetilde{X}_{1l} \\
                     &= \frac{p_0 p_1}{2}  + p_0 q_1 + q_0 q_1 \pi_{x1} \\
                     &= \pi_{\widetilde{U}1} \\
  \sigma^2(\widetilde{U}) &= (n_0 n_1)^{-1}
                             \left\{ \pi_{\widetilde{U}1} (1 - \pi_{\widetilde{U}1}) +
                             (n_0 - 1) \left(\pi_{\widetilde{U}2} - \pi_{\widetilde{U}1}^2  - \frac{p^2_0 p_1}{12} \right)
                             \right.\\
                          &\hphantom{(n_0 n_1)^{-1} left\{ }
                          \left.
                            + (n_1 - 1) \left(\pi_{\widetilde{U}3} - \pi_{\widetilde{U}1}^2  - \frac{p_0 p^2_1}{12}\right)
                            - \frac{p_0 p_1}{4}\right\}
  \intertext{with}
  \pi_{\widetilde{U}2} &= p^2_0 q_1 + \frac{p^2_0 p_1}{3} + 2 p_0 q_0 q_1 \pi_{X1} + q^2_0 q_1 \pi_{X2} \\
  \pi_{\widetilde{U}3} &= p_0 q^2_1 + \frac{p_0 p^2_1}{3} + p_0 p_1 q_1 + q_0 q^2_1 \pi_{X3}
                          \mbox{, where } \pi_{X1}, \pi_{X2}, \pi_{X3} \mbox{ are as in the untied case.}
\end{align*}

Again, $\mu(\widetilde{U})$ and $\sigma(\widetilde{U})$ under the null
hypothesis or under the alternative can be computed if assumptions about the
distributions of quantitative endpoint and event time are made. As previously,
we consider the boundary of the null hypothesis, i. e.
$\mu_0(\widetilde{U}) = \text{P}(\widetilde{X}_{0k} <
\widetilde{X}_{1l}) = \frac{1}{2} - \varepsilon$ when assessing the power of
the test. The power is given by

\begin{align*}
  1 - \beta &= \text{P} \left(\frac{\widetilde{U} - \mu_0(\widetilde{U})}{\sigma_0(\widetilde{U})}  >
                                 z_{1-\alpha} \mid H_1 \right) \\
            &= \Phi\left(z_{\alpha}\frac{\sigma_0(\widetilde{U})}{\sigma_1(\widetilde{U})} +
                            \frac{\mu_1(\widetilde{U}) - \mu_0(\widetilde{U})}{\sigma_1(\widetilde{U})} \right)
\end{align*}

If the specific alternative considered is equivalence of treatments, i. e.
$\widetilde{X}_{0k}$ and $\widetilde{X}_{1l}$ are from the same distribution,
we have $p_0 = p_1 = p, q_0 = q_1 = q, \pi_{X1} = 1/2,
\pi_{X2} = \pi_{X3} = 1/3$ and hence
$\pi_{\widetilde{U}1} = 1/2, \pi_{\widetilde{U}2} = \pi_{\widetilde{U}3} = 1/3 $.
Consequently, $\mu_1(\widetilde{U}) = \text{P}(\widetilde{X}_{0k} < \widetilde{X}_{1l}) = 1/2$
and \\
$\sigma^2_1(\widetilde{U}) = \left\{ (n_0 + n_1 +1) -
              p^2 \left[ 3 +(n_0 + n_1 -2) p \right] \right\} / (12 n_0 n_1)$.\\
The power is then given by

\begin{align*}
  1 - \beta &= \Phi\left(z_{\alpha}\frac{\sigma_0(\widetilde{U})}{\sigma_1(\widetilde{U})} +
                          \frac{\varepsilon}{\sigma_1(\widetilde{U})} \right) \\
            &= \textstyle{\Phi\left(z_{\alpha}
               \sqrt\frac{
               \left\{ \pi_{\widetilde{U}1} (1 - \pi_{\widetilde{U}1})
                     + (n_0 - 1)( \pi_{\widetilde{U}2} - \pi^2_{\widetilde{U}1} - \frac{p_0^2 p_1}{12})
                     + (n_1 - 1)( \pi_{\widetilde{U}3} - \pi^2_{\widetilde{U}1} - \frac{p_0 p_1^2}{12})
                     - \frac{p_0 p_1}{4}
              \right\}}{\left\{ (n_0 + n_1 +1) -
              p^2 \left[ 3 +(n_0 + n_1 -2) p \right] \right\} / 12 } \right.} \\
            &{\hphantom{= \Phi\bigg(}} + \left.
            \varepsilon \sqrt\frac{12}{(n_0 + n_1 +1) -
              p^2 \left[ 3 +(n_0 + n_1 -2) p \right]} \right)
\end{align*}


\section{Application}
\label{sec:Application}
For the clinical application we assume that the RV/LV reduction is approximately
normally distributed with common variance for both treatment groups:
$X_i \sim \mathcal{N} ( \mu_i, \sigma^2 ), (i = 0, 1)$. Under the standard
treatment RV/LV reduction is expected to be $\mu_0 = 0.3$ on average with
standard deviation $\sigma= 0.1$. A difference of $\varepsilon^{*} = 0.05$ is
deemed acceptable. As the quantitative endpoint is assumed to be normally
distributed, the acceptable difference can be expressed as a multiple $c$
of the standard deviation, here $c = 0.5$. This would be used as non-inferiority
margin in a parametric test if no censoring by death occurred. The corresponding
parametric hypotheses are
$H_0: \mu_1 \leq \mu_0 - \varepsilon^{*}  = \mu_0 - c \sigma $ and
$H_1: \mu_1 > \mu_0 - \varepsilon^{*} = \mu_0 - c \sigma $ with $\mu_0 = 0.3$,
$\sigma = 0.1$ and $c = 0.5$. We chose $\alpha = 0.025$ for this one-sided test.
We explore the power as a function of sample size and the probabilities of death
in both treatment groups for the specific alternative of equivalence of
treatments. The ratio of $n_0 : n_1$ is chosen as $1 : 2 $ in our application.

In a first step we determine the non-parametric non-inferiority margin $\tilde{\varepsilon}$
for the case of no censoring by death such that
$H_0: P(X_0 < X_1) \leq \frac{1}{2} - \tilde{\varepsilon}$ and
$H_1: P(X_0 < X_1) >  \frac{1}{2} - \tilde{\varepsilon}$.

As $\Phi(x)$ is monotonically increasing we have
$P(X_0 < X_1) = \Phi((\mu_1 - \mu_0)/\sqrt{2}\sigma) \leq
\Phi(- \varepsilon^{*}/ \sqrt{2}\sigma) = \Phi(-c/\sqrt{2}) $ under $H_0$ .
Therefore, if no censoring by death occured, $\tilde{\varepsilon}$ would be
chosen such that $1/2 - \tilde{\varepsilon} = \Phi(- c/\sqrt{2}) $ and hence
with $c = 0.5$ we chose  $\tilde{\varepsilon} = 1/2 - \Phi(- 1/\sqrt{8})
\approx $ \Sexpr{round(0.5 - pnorm(-1/sqrt(8)), 4)}.

All calculations were performed using
\Sexpr{noquote(sessionInfo.txt$R.version$version.string)},
packages {\tt{knitr, xtable, latex2exp, }} and {\tt{flexsurv}}.

\subsection{Untied case}
\label{sec:AppUntied}
For the untied case we assume that the time to death follows an exponential
distribution, i. e. $T_i \sim exp(\lambda_i), (i=0,1)$ and hence
$q_i = exp(-\lambda_i \tau)$. Without loss of generality we can assume that
$\tau = 1$. To determine the overall non-inferiority margin $\varepsilon$, again
consider the boundary of the null hypothesis, i. e.
$\text{P}(\widetilde{X}_{0k} < \widetilde{X}_{1l}) = 1/2 - \varepsilon $.

\begin{align*}
\text{P}(\widetilde{X}_{0k}  < \widetilde{X}_{1l}) &= p_0 p_1 \text{P}(T_{0k} < T_{1l}) + p_0 q_1 + q_0 q_1 P(X_{0k} < X_{1l}) \\
                                  &= p_0 p_1 \frac{\lambda_0}{\lambda_0 + \lambda_1} + p_0 (1-p_1)  + (1 - p_0)(1 - p_1)(\frac{1}{2} - \tilde{\varepsilon}) \\
                                  &= \frac{1}{2} - \left( (1-p_0)(1-p_1) \tilde{\varepsilon} + \gamma p_0 p_1 + \frac{1}{2} (p_1 - p_0) \right) \mbox{ with } \gamma = \frac{1}{2} - \frac{\lambda_0}{\lambda_0 + \lambda_1} \\
\intertext{and hence, as $\tilde{\varepsilon} = 1/2 - \Phi(-c/\sqrt{2})$,}
 \varepsilon &= (1-p_0)(1-p_1)\tilde{\varepsilon}  + \gamma p_0 p_1 + (p_1 - p_0)/2 \\
             &= (1-p_0)(1-p_1) (1/2 - \Phi(-c/\sqrt{2})) + \gamma p_0 p_1 + (p_1 - p_0)/2
\intertext{For $p_0 = p_1 = p$ this simplifies to}
 \varepsilon &= (1-p)^2 \tilde{\varepsilon} = (1-p)^2 (1/2 - \Phi(-c/\sqrt{2})).
\end{align*}

<<powerplot_untied, fig.lp="fig:", fig.cap = 'Power and sample size for untied case with sample size allocation for reference to new treatment 1:2. In each panel a  different relative risk (RR) of death in the new treatment group (risk $p_1$) compared to the reference group (risk $p_0$) is assumed under the null hypothesis. The power is computed for the alternative of equivalence of both treatments. Different line types correspond to different risks in the reference group.', echo=FALSE, eval=TRUE, results='hide', out.width='0.8\\linewidth',out.height='0.9\\linewidth'>>=
# explore power in untied case for specific situation described above, varying the
# probabiliy of death
# sample size in each group
n_0 <- seq(1:150)
n_1 <- 2*n_0

# chosen probabilities of death
chosen_ps <- c(0, 0.01, 0.02, 0.05, 0.1, 0.2)
l_chosen_ps <- length(chosen_ps) # number of chosen probabilities of death

# chosen relative risks (treatment(1)  vs control(0) under H0)
chosen_risks <- c(1, 1.2, 1.75, 2.5)
l_chosen_risks <- length(chosen_risks) # number of chosen risks
figure_elements <- LETTERS[1:l_chosen_risks]

# vector to hold sample sizes for power 80%
samplesize_80 <- c(rep(NA, l_chosen_ps))
names(samplesize_80) <- as.character(chosen_ps)

# matrix to hold power for all combinations of probabilities of death and sample size
MyPower.Matrix <- matrix(rep(NA, length(n_0)*l_chosen_ps), ncol = l_chosen_ps)
dimnames(MyPower.Matrix)[[2]] <- as.character(chosen_ps)

# vector to hold non-inferiority margins corresponding to chosen situation
epsilon_ps <- c(rep(NA, l_chosen_ps))
names(epsilon_ps) <- as.character(chosen_ps)

# vector of datasets to hold combination of chosen p's, computed epsilon's and
# samplesizes for power 80%
results <- list()

# construct panel of 2x2 graphs, this only works fine if four graphs are produced
par(mfrow = c(2, 2))

for (RR in seq_along(chosen_risks)) {
  for (p in chosen_ps) {
    MyPower <- power_gr(alpha = 0.025,
                        p_0_0 = p, p_1_0 = chosen_risks[RR]*p,
                        p_0_1 = p, p_1_1 = p,
                        mu_0_0 = 0.3, mu_1_0 = 0.25, mu_0_1 = 0.3, mu_1_1 = 0.3,
                        sigma = 0.1, tau = 1,
                        n_0 = n_0, n_1 = n_1,
                        verbose = FALSE, tied = FALSE)
    MyPower.Matrix[, as.character(p)] <- MyPower$power
    epsilon_ps[as.character(p)] <- MyPower$epsilon
    index <- min(which(MyPower$power >= 0.8))
    samplesize_80[as.character(p)] <- n_0[index] + n_1[index]
  }
  results[[RR]] <- data.frame(chosen_ps, epsilon_ps, samplesize_80)
  if (RR == 1) {
    SummaryPower.Matrix <- MyPower.Matrix

  }else{
    SummaryPower.Matrix <- rbind(SummaryPower.Matrix, MyPower.Matrix)
  }

  # plot power as function of sample size and p
  # first establish empty plot with axes
  plot(NULL, xlab = "Total sample size", ylab = "Power",
       xlim = c(0, max(n_0) + max(n_1)), ylim = c(0, 1))
  my.lt <- c(1:6)     # define line types for graph
  my.col <- c(grey.colors(l_chosen_ps, start = 0, end = 0.6, gamma = 2.2))
                       # define 'colors' for graph
  # plots for p_0 ^= 0
  for (i in seq_along(chosen_ps[2:l_chosen_ps])) {
    lines(x = n_0 + n_1, y = MyPower.Matrix[, as.character(chosen_ps[i + 1])],
          lwd = 2, lty = my.lt[i + 1], col = my.col[i + 1])
  }
  # plots for p_0 = 0 (black line should always be visible)
  lines(x = n_0 + n_1, y = MyPower.Matrix[, "0"], type = "l", lwd = 2, lty = 1,
        col = "black")

  legend("bottomright", title = TeX("$p_1 = $ RR $ \\cdot p_0$"),
         legend = as.character(chosen_ps), inset = 0.05, bty = "n", lwd = 2,
         lty = my.lt, col = my.col )
  legend("topleft", legend = figure_elements[RR], cex = 1.5, inset = -0.05,
         bty = "n")
  title(main = paste0("RR = ", as.character(chosen_risks[RR])))
}

# plot  figure to TIF-file
tiff(filename = "SchmidtmannKonstantinidesBinder_Figure1.tif",
     width = 17, height = 17, units = "cm", pointsize = 12,
     bg = "white", res = 300, family = "", restoreConsole = TRUE,
     type = c("windows", "cairo"))

# construct panel of 2x2 graphs, this only works fine if four graphs are produced
par(mfrow = c(2, 2))

for (RR in seq_along(chosen_risks)) {
  for (p in chosen_ps) {
    MyPower <- power_gr(alpha = 0.025,
                        p_0_0 = p, p_1_0 = chosen_risks[RR]*p,
                        p_0_1 = p, p_1_1 = p,
                        mu_0_0 = 0.3, mu_1_0 = 0.25, mu_0_1 = 0.3, mu_1_1 = 0.3,
                        sigma = 0.1, tau = 1,
                        n_0 = n_0, n_1 = n_1,
                        verbose = FALSE, tied = FALSE)
     MyPower.Matrix[, as.character(p)] <- MyPower$power
     epsilon_ps[as.character(p)] <- MyPower$epsilon
     index <- min(which(MyPower$power >= 0.8))
     samplesize_80[as.character(p)] <- n_0[index] + n_1[index]
  }
  results[[RR]] <- data.frame(chosen_ps, epsilon_ps, samplesize_80)
  if (RR == 1) {
    SummaryPower.Matrix <- MyPower.Matrix

  }else{
    SummaryPower.Matrix <- rbind(SummaryPower.Matrix, MyPower.Matrix)
  }

  # plot power as function of sample size and p
  # first establish empty plot with axes
  plot(NULL, xlab = "Total sample size", ylab = "Power",
       xlim = c(0, max(n_0) + max(n_1)), ylim = c(0, 1))
  my.lt <- c(1:6)     # define line types for graph
  my.col <- c(grey.colors(l_chosen_ps, start = 0, end = 0.6, gamma = 2.2))
                       # define 'colors' for graph
  # plots for p_0 ^= 0
  for (i in seq_along(chosen_ps[2:l_chosen_ps])) {
    lines(x = n_0 + n_1, y = MyPower.Matrix[, as.character(chosen_ps[i + 1])],
          lwd = 2, lty = my.lt[i + 1], col = my.col[i + 1])
  }
  # plots for p_0 = 0 (black line should always be visible)
  lines(x = n_0 + n_1, y = MyPower.Matrix[, "0"], type = "l", lwd = 2, lty = 1,
        col = "black")

  legend("bottomright", title = TeX("$p_1 = $ RR $ \\cdot p_0$"),
         legend = as.character(chosen_ps), inset = 0.05, bty = "n", lwd = 2,
         lty = my.lt, col = my.col )
  legend("topleft", legend = figure_elements[RR], cex = 1.5, inset = -0.05,
         bty = "n")
  title(main = paste0("RR = ", as.character(chosen_risks[RR])))
}
dev.off()

# selects every 10th element of the sample power matrix
ii <- seq(10,(4*length(n_0)),10)
TableAppendix <-  SummaryPower.Matrix[ii, ]

ii <- 3 * seq(10,length(n_0),10)
RR_label <- c(chosen_risks[1], rep("", length(ii) - 1),
              chosen_risks[2], rep("", length(ii) - 1),
              chosen_risks[3], rep("", length(ii) - 1),
              chosen_risks[4], rep("", length(ii) - 1))
TableAppendix <- data.frame(RR_label,
                            rep(ii, 4),
                            TableAppendix)

@

<<power_tied, echo=FALSE, eval=TRUE>>=
# explore power in tied case for specific situation described above, varying the
# probabiliy of death
# constants as above
# sample size in each group: n_0, n_1
# chosen probabilities of death: chosen_ps and l_chosen_ps
# chosen relative risks: chosen_risks, l_chosen_risks

# vector to hold sample sizes
samplesize_80_tied <- c(rep(NA, l_chosen_ps))
names(samplesize_80_tied) <- as.character(chosen_ps)

# matrix to hold power for all combinations of probabilities of death and sample
# size
MyPower.Matrix.Tied <- matrix(rep(NA, length(n_0)*l_chosen_ps),
                              ncol = l_chosen_ps)
dimnames(MyPower.Matrix.Tied)[[2]] <- as.character(chosen_ps)

# vector to hold non-inferiority margins corresponding to chosen situation
epsilon_ps_tied <- c(rep(NA, l_chosen_ps))
names(epsilon_ps_tied) <- as.character(chosen_ps)

# vector of datasets to hold combination of chosen p's, computed epsilon's and
# sample sizes for power 80%
results_tied <- list()

for (RR in seq_along(chosen_risks)) {
  for (p in chosen_ps) {
    MyPower.Tied <- power_gr(alpha = 0.025,
                        p_0_0 = p, p_1_0 = chosen_risks[RR]*p,
                        p_0_1 = p, p_1_1 = p,
                        mu_0_0 = 0.3, mu_1_0 = 0.25, mu_0_1 = 0.3, mu_1_1 = 0.3,
                        sigma = 0.1, tau = 1,
                        n_0 = n_0, n_1 = n_1,
                        verbose = FALSE, tied = TRUE)
     MyPower.Matrix.Tied[, as.character(p)] <- MyPower.Tied$power
     epsilon_ps_tied[as.character(p)] <- MyPower.Tied$epsilon
     index <- min(which(MyPower.Tied$power >= 0.8))
     samplesize_80_tied[as.character(p)] <- n_0[index] + n_1[index]
  }
  results_tied[[RR]] <- data.frame(chosen_ps, epsilon_ps_tied, samplesize_80_tied)
  if (RR == 1) {
    SummaryPower.Matrix.Tied <- MyPower.Matrix.Tied

  }else{
    SummaryPower.Matrix.Tied <- rbind(SummaryPower.Matrix.Tied, MyPower.Matrix.Tied)
  }
}

# selects every 10th element of the sample power matrix
ii <- seq(10,(4*length(n_0)),10)
TableAppendix.Tied <-  SummaryPower.Matrix.Tied[ii, ]

ii <- 3 * seq(10,length(n_0),10)
TableAppendix.Tied <- data.frame(RR_label,
                            rep(ii, 4),
                            TableAppendix.Tied)
@

In figure \ref{fig:powerplot_untied} the solid black line is identical in all
panels; it depicts the situation when no censoring due to death occurs. Figure
\ref{fig:powerplot_untied}A describes the situation when mortality is the same
in both treatment groups. For given sample size power decreases with an increase
in probability of censoring by death. Figure \ref{fig:powerplot_untied}B also
exhibits this effect, however, here the risk of censoring by death is 20\%
higher in the new treatment group under the null hypothesis and the power is
less affected by the risk of censoring by death. If $\lambda_0$ and
$\lambda_1$ are such that
$\lambda_0 / (\lambda_0 + \lambda_1) = 1/2 - \tilde{\varepsilon}$ then

\begin{align*}
\text{P}(\widetilde{X}_{0k}  < \widetilde{X}_{1l}) &= p_0 p_1 (1/2 - \tilde{\varepsilon}) + p_0 (1-p_1)  + (1 - p_0)(1 - p_1)(1/2 - \tilde{\varepsilon}) \\
   &= (1/2 - \tilde{\varepsilon})(1 + p_0 - p_1) - 2 \tilde{\varepsilon}p_0 p_1 \\
   &\approx 1/2 - \tilde{\varepsilon} \mbox{ if } p_0, p_1 \ll 1
\end{align*}

This effect can be seen in figure \ref{fig:powerplot_untied}C where the power
for the smaller probabilities for death is very similar to the situation where
there is no censoring by death. In figure \ref{fig:powerplot_untied}D the
assumption is that under the null hypothesis the risk for death ist 2.5 fold in
the new treatment group compared to the reference group. In this case the
computed power increases with increasing probability of censoring due to death.
Values displayed in figure \ref{fig:powerplot_untied} are also provided in table
\ref{tab:AppendixUntied} in the appendix.

\subsection{Tied case}
\label{sec:AppTied}
For the tied case it suffices to make an assumption about the probability of
death in each group under the null hypothesis. To determine the overall
non-inferiority margin we again consider the boundary of the null hypothesis
$\text{P}(\widetilde{X}_{0k}  < \widetilde{X}_{1l}) = 1/2 - \varepsilon $.


\begin{align*}
 \text{P}(\widetilde{X}_{0k}  < \widetilde{X}_{1l}) &= p_0 p_1 / 2 + p_0 q_1 + q_0 q_1 P(X_{0k} < X_{1l}) \\
    &= p_0 - p_0 p_1/2 + (1 - p_0)(1 - p_1)/2 - (1 - p_0)(1 - p_1) \tilde{\varepsilon}\\
    &= 1/2 - (p_1 - p_0)/2 - (1 - p_0)(1 - p_1) \tilde{\varepsilon}
\intertext{and hence, again considering that $\tilde{\varepsilon} = 1/2 - \Phi(-c/\sqrt{2})$,}
\varepsilon &= (p_1 - p_0)/2 + (1 - p_0)(1 - p_1) \tilde{\varepsilon} \\
            &= (p_1 - p_0)/2 + (1 - p_0)(1 - p_1) (1/2 - \Phi(-c/\sqrt{2}))
\intertext{For $p_0 = p_1 = p$ this also simplifies to}
\varepsilon &= (1-p)^2 \tilde{\varepsilon} = (1-p)^2 (1/2 - \Phi(-c/\sqrt{2})).
\end{align*}


<<xtableA, echo=FALSE, results="asis">>=
# This chunk produces table 1 in the paper
results_for_table <- data.frame(
  chosen_ps = double(),
  epsilon_ps = double(),
  samplesize_80 = double(),
  epsilon_ps_tied = double(),
  samplesize_80_tied = double()
)
RR_column <- character()
for (RR in seq_along(chosen_risks)) {
  results_for_table <- rbind(results_for_table,
                             cbind(results[[RR]],
                                   results_tied[[RR]]$epsilon_ps_tied,
                                   results_tied[[RR]]$samplesize_80_tied))
  RR_column <- c(RR_column, as.character(chosen_risks[RR]), rep(" ", l_chosen_ps - 1))
}
results_for_table <- cbind(RR_column, results_for_table)

colnames(results_for_table) <- c("RR", "$p_0$",
                                 "$\\varepsilon^{\\text{(untied)}}$",
                                 "$n^{\\text{(untied)}}_{\\text{total}}$",
                                 "$\\varepsilon^{\\text{(tied)}}$",
                                 "$n^{\\text{(tied)}}_{\\text{total}}$")
print(xtable(results_for_table,caption = 'Effective non-inferiority margin and total samples size as function of relative risk (RR) of death in the new treatment group (risk $p_1$) compared to the reference group (risk $p_0$) for given $\\alpha =0.025$ and power of 80\\% with sample size allocation for reference to new treatment 1:2.',label = "tab:test",digits = c(0, 0, 2, 3, 0, 3, 0)),
      include.rownames = FALSE,
      caption.placement = "top",
      sanitize.colnames.function = function(x){x})
@

In table \ref{tab:test} the effective non-inferiority margin $\varepsilon$ for
the Wilcoxon-Mann-Whitney test for non-inferiority and the total number of cases
needed to obtain a power of 80\% at signifcance level $\alpha =0.025$ are
shown as function of the probability of death in the reference group $p_0$ and
the relative risk $RR$ of death in the new treatment group compared to the
reference group under $H_0$.

In our example there is little difference in power between the tied case and
the untied case for given sample size. Specifically, effective non-inferiority
margins and sample sizes are identical in the tied and untied case if the
probability of death is identical in both treatment groups. Values for the tied
case corresponding to the situation depicted in figure \ref{fig:powerplot_untied}
are shown in table \ref{tab:AppendixTied} in the appendix.

So far we based the choice of the non-inferiority margin on assumptions
concerning the quantitative endpoint, starting from the assumption of zero
probability of censoring by death in both groups under the null hypothesis.
This was extended to investigating several non-zero constellations of
probability of censoring by death in both treatment groups. An alternative would
be to start from the assumption of identical distributions of the quantitative
endpoint and to derive the non-inferiority margin from an acceptable difference
in risks of death. In this case $c = 0$ and the formula for the non-inferiority
margin $\varepsilon$ in the tied case simplifies to
$\varepsilon = (p_1 - p_0) / 2$. In the untied case the formula for the
non-inferiority margin is given by
$\varepsilon = \gamma p_0 p_1 + (p_1 - p_0) / 2$.


\subsection{Simulation}
\label{sec:Sim}

<<define_number_of_replicates, echo=FALSE, eval=TRUE, cache = TRUE>>=
# number of replicates
# (mini-chunnk because the number of replicates is needed in the next section)
r <- 10000
@

To assess whether the accuracy of the power formula holds if the time to death
does not follow an exponential distribution we performed a small simulation
study. In this we assumed that time to event follows a log-logistic distribution
with hazard $h(t; \alpha, \beta) = \beta t^{\beta - 1} / (\alpha^{\beta} + t^{\beta})$,
with scale parameter $\alpha > 0 $ and shape parameter $\beta > 0$. Scale
$\alpha$ was chosen such that corresponding cumulative incidence functions
matched the pre-specified probabilities of death at time $\tau = 1$. While the
shape $\beta$ was assumed to be identical in both groups, it was varied between
simulation scenarios assuming values 0.8, 1.0, and 1.2.
The probabilities of censoring by death and the relative risks between the
reference group and new treatment group were chosen as in \ref{sec:AppTied}, the
total sample size was chosen as determined in table \ref{tab:test}. \Sexpr{r}
datasets were generated and evaluated in each scenario. The results of the
simulation are shown in table \ref{tab:simresult} in the appendix.

<<sim, echo=FALSE, eval=TRUE, cache = TRUE>>=
# set seed for random number generation for reproducibility
set.seed(270318)
# define scenarios - use parameters from above
n_total <- c(results[[1]]$samplesize_80,
             results[[2]]$samplesize_80,
             results[[3]]$samplesize_80,
             results[[4]]$samplesize_80)
n_0 <- n_total / 3
n_1 <- 2 * n_0
# number of scenarios considered
no_scenarios <- length(n_total)
# time at which quantitative endpoint is determined
tau <- 1
# risk of death by time \tau under null hypothesis in reference group
# p_0_0 = 0, 0.01, 0.02, 0.05, 0.1, 0.2
p_0_0 <- rep(chosen_ps, 4)
# set vector of relative risks
RR <-  rep(chosen_risks, rep(6, 4))
# risk of death by time \tau in new treatment group under null hypothesis
p_1_0 <- RR * p_0_0
# risks of death by time \tau under alternative hypothesis
p_0_1 <- p_0_0    # 0, 0.01, 0.02, 0.05, 0.1, 0.2
p_1_1 <- p_0_1    # 0, 0.01, 0.02, 0.05, 0.1, 0.2
# means of quantitative variable under null hypothesis
mu_0_0 <- 0.3
mu_1_0 <- 0.25
# means of quantitative variable under alternative hypothesis
mu_0_1 <- 0.3
mu_1_1 <- 0.3
# variance of quantitative variable
sigma <- 0.1
# consider untied case
tied <- FALSE
verbose <- FALSE

# define column names for result matrices
my_colnames <- paste0("RR = ", RR, ", p_0_0 = ", p_0_0)
my_colnames_tied <- paste0("RR = ", RR, ", p_0_0 = ", p_0_0, " (tied)")
# matrix to hold test statistics returned by function wilcox.test (untied and
# tied case)
wx <- matrix(nrow = r, ncol = no_scenarios)
wx_tied <- matrix(nrow = r, ncol = no_scenarios)
# matrix to hold Mann-Whitney statistics (derived from rank sum) (untied and
# tied case)
mw <- matrix(nrow = r, ncol = no_scenarios)
mw_tied <- matrix(nrow = r, ncol = no_scenarios)
# matrix to hold standardised Mann-Whitney statistics (untied and tied case)
mw_standardised <- matrix(nrow = r, ncol = no_scenarios)
mw_standardised_tied <- matrix(nrow = r, ncol = no_scenarios)
# matrix to hold p-values corresponding to null hypotheses given by parameters
# n_0, n_1, p_0_0, p_1_0, mu_0_0, mu_1_0, sigma, tau  (untied and tied case)
p_values <- matrix(nrow = r, ncol = no_scenarios)
p_values_tied <- matrix(nrow = r, ncol = no_scenarios)
# assign column names to result matrices
colnames(wx) <- my_colnames
colnames(wx_tied) <- my_colnames_tied
colnames(mw) <- my_colnames
colnames(mw_tied) <- my_colnames_tied
colnames(mw_standardised) <- my_colnames
colnames(mw_standardised_tied) <- my_colnames_tied
colnames(p_values) <- my_colnames
colnames(p_values_tied) <- my_colnames_tied


# choose shape parameters for log-logistic distribution
beta <- c(0.8, 1.0, 1.2)
# matrix to hold power obtained in each scenario (defined by p_0_0, RR and
# n_total) for each value of beta (untied and tied case)
power_beta <- matrix(nrow = no_scenarios, ncol = length(beta))
power_beta_tied <- matrix(nrow = no_scenarios, ncol = length(beta))
colnames(power_beta) <- beta
colnames(power_beta_tied) <- beta

# beta_loop
for (b in seq_along(beta)) {
  # scenario loop
  for (sc in seq(1:no_scenarios)) {
    # replicate loop
    for (i in seq(1:r)) {
      # generate death censored observations under alternative hypothesis - untied
      results_gen_dco <- generate_dco(n_0 = n_0[sc], n_1 = n_1[sc],
                                      p_0 = p_0_1[sc], p_1 = p_1_1[sc],
                                      mu_0 = mu_0_1, mu_1 = mu_1_1,
                                      sigma = sigma,
                                      tau = 1,
                                      b_0 = beta, b_1 = beta,
                                      verbose = FALSE, tied = FALSE)
      # compute rank sum statistics based on $\tilde{X_{0k}}$ and $\tilde{X_{1l}}$
      wx[i, sc] <- wilcox.test(results_gen_dco$xx_1,
                               results_gen_dco$xx_0)$statistic
      # generate death censored observations - tied
      results_gen_dco_tied <- generate_dco(n_0 = n_0[sc], n_1 = n_1[sc],
                                           p_0 = p_0_1[sc], p_1 = p_1_1[sc],
                                           mu_0 = mu_0_1, mu_1 = mu_1_1,
                                           sigma = sigma,
                                           tau = 1,
                                           b_0 = beta, b_1 = beta,
                                           verbose = FALSE, tied = TRUE)
      # compute rank sum statistics based on $\tilde{X_{0k}}$ and $\tilde{X_{1l}}$
      wx_tied[i, sc] <- wilcox.test(results_gen_dco_tied$xx_1,
                                    results_gen_dco_tied$xx_0,
                                    exact = FALSE)$statistic
    }
    # obtain Mann-Whitney statistics
    mw[ ,sc] <- wx[ ,sc] / (n_0[sc] * n_1[sc])
    mw_tied[ ,sc] <- wx_tied[ ,sc] / (n_0[sc] * n_1[sc])
    # compute moments under H_0
    moments_H_0 <- moments(n_0 = n_0[sc], n_1 = n_1[sc],
                           p_0 = p_0_0[sc], p_1 = p_1_0[sc],
                           mu_0 = mu_0_0, mu_1 = mu_1_0,
                           sigma = sigma, tau = tau,
                           verbose = verbose, tied = tied)
    moments_H_0_tied <- moments(n_0 = n_0[sc], n_1 = n_1[sc],
                                p_0 = p_0_0[sc], p_1 = p_1_0[sc],
                                mu_0 = mu_0_0, mu_1 = mu_1_0,
                                sigma = sigma, tau = tau,
                                verbose = verbose, tied = tied)
    # obtain standardised Mann-Whitney statistic
    mw_standardised[ ,sc] <- (mw[ ,sc] - moments_H_0$mu_u) / moments_H_0$sigma_u
    mw_standardised_tied[ ,sc] <- (mw_tied[ ,sc] - moments_H_0_tied$mu_u) / moments_H_0_tied$sigma_u
  }
  # compute p-values for each  in each scenario
  p_values <- pnorm(mw_standardised, lower.tail = FALSE)
  p_values_tied <- pnorm(mw_standardised_tied, lower.tail = FALSE)
  # obtain power by counting how many time a p-value is <= 0.025
  power_beta[, b] <- apply(p_values <= 0.025, 2, sum) / r
  power_beta_tied[, b] <- apply(p_values_tied <= 0.025, 2, sum) / r
}

# data frame to hold power determined in each scenario for each beta
power_results <- data.frame(
  RR_column,
  p_0_0,
  n_total,
  power_beta,
  power_beta_tied
)
@


In the simulation, the power was found to be close to the nominal power of 80\%
in all scenarios studied. In the untied case, the estimated power was
\Sexpr{round(100*mean(power_beta), 1)} \% on average, range
\Sexpr{round(100*min(power_beta), 1)} \% - \Sexpr{round(100*max(power_beta), 1)} \%.
In the tied case we observed average power
\Sexpr{round(100*mean(power_beta_tied), 1)} \%, range
\Sexpr{round(100*min(power_beta_tied), 1)} \% - \Sexpr{round(100*max(power_beta_tied), 1)} \%.
If the power is 80 \%, i. e. 0.8, with \Sexpr{r} replicates, the standard error
of the estimated power is \Sexpr{100* sqrt(0.8 * 0.2 / r)} percentage points.
This supports the conclusion that deviations of the observed power from 80 \% are
not substantinal but due to random noise.


\section{Discussion}
\label{sec:Discussion}
In section \ref{sec:Power} we derived power formulae for the Wilcoxon-Mann-Whitney
test for non-inferiority for the situation were observations may be censored due
to death, relying on the work of Matsouaka and Betensky (2015) for the ordinary
Wilcoxon-Mann-Whitney test in the presence of death censored observations. Both,
the untied situation in which time of death is taken into account for ranking and
the tied situation in which all deaths are ranked identically were considered.
The formulae use approximations which seem to give valid results.

We applied the derived formulae to an example from cardiology and found in this
situation that there is little to no difference in power for given sample size
between the tied and untied case. In the simulation we also found little
difference between the tied and untied case.

As expected, power decreases with increasing probability of death -- while the
relative risk in the new treatment group compared to the reference group is not
too high. This effect is most pronounced if the probability of death is identical
in both groups under the non-inferiority null hypothesis. It decreases with
increasing relative risk in the the new treatment group compared to the reference
group. We also observed that sample size and power are hardly affected by
censoring due to death if
$\lambda_0/(\lambda_0 + \lambda_1) \approx 1/2 - P(X_0 < X_1)$ where
$\lambda_0, \lambda_1$ are the hazards -- given exponential distribution of time
to death -- and $P(X_0 < X_1)$ the probability that the quantiative outcome tends
to smaller values in the reference distribution. If the risk of death in the new
treatment group is much higher  than in the reference group the effects are
reversed when applying the formulae, i. e. power increases with increasing
probability of death due to censoring. This effect could be corroborated in the
simulation.

The choice of non-inferiority margin presents an additional challenge when global
ranks are used to tackle censoring by death. Wellek (2010) gives some general
suggestions for the choice of the equivalence or non-inferiority margin in the
non-parametric situation, while Munzel and Hauschke (2003) and Zhang et al (2015)
have investigated the special case of ordered categorical data. However, neither
of these approaches can be transferred directly to the problem presented here.

Furthermore, Munzel (2009) has suggested to choose the non-inferiority margin
such that establishing non-inferiority of a new treatment to a reference
treatment would also allow to conclude that the new treatment is better than
placebo. This requires either a three-armed trial including placebo in addition
to the new treatment and the reference treatment or reliable data from a previous
study in which the reference treatment was shown to be better than placebo.
The former is -- as Munzel has pointed out clearly -- unethical in a situation
where death is a realistic threat. The latter would require both information on
the quantitative endpoint and probability of death for the no treatment option --
which is not available in our context.

Therefore, in our example, we chose to derive the non-inferiority margin based
on clinical considerations about equivalence in the outcome of the quantitative
endpoint. As we could assume normality for the quantitative outcome, we used this
to derive a joint non-inferiority margin. If normality cannot be assumed
other criteria would have to be used.

Using a joint non-inferiority margin for both, the probability of death and the
quantitative outcome, may not always be appropriate. It is possible to use
separate margins when deriving the power formula or to focus on an acceptable
non-inferiority margin for the probability of death. However, the separate
margins will effectively be combined into one overall margin.

In our application we delibarately considered only relatively small probabilities
of censoring due to death in the reference group and small to moderate relative
risks in the new treatment group compared to the reference group. If there is a
substantial proportion of early deaths, mortality is likely to be a more relevant
primary endpoint than any functional parameter. Also, allowing much higher risk
of death as non-inferior does not seem to be a reasonable assumption for clinical
trials, either.

As usual when using any combination of endpoints, there may be conflicting
results, i. e. there may be an advantage of the new treatment with respect to
the quantitative outcome and a disadvantage with respect to death or vice versa.
This possibility needs careful consideration.

Another possibility would be to assign different weights to time to death and
the quantitative outcome such as suggested by Matsouaka et al (2016). This would
allow to prevent to a certain extent to take too high risks of death when the
new treatment is beneficial only with respect to the quantitative endpoint.

However, our focus is on finding an appropriate way of including cases with
missing quantitative outcome due to death in the analyses. In the end, clinical
judgement is necessary and in a clinical trial the two outcomes have to be
carefully assessed when the non-inferiority null hypothesis is rejected.

The presented approach does not allow for inclusion of covariates. If it is
important to include covariates rank regression models could be applied.
For such an approach the power formulae presented here will at best give a rough
estimate of required power or sample size. A separate power analysis will be
needed in this case.

Also, so far risk of death and underlying values of the quantitative variable
have been assumed to be independent. This could be relaxed by considering joint
models that take into account shared or correlated frailty.

\section*{Appendix}
\subsection*{A.1.\enspace Untied case}
In table \ref{tab:AppendixUntied} an abridged list of values used to obtain
figure \ref{fig:powerplot_untied} is shown. Total sample size is given in steps
of 30; only sample sizes for which at least one observed power is less than or
equal to 0.95 are displayed.
<<xtableC, echo=FALSE, results="asis">>=
# This chunk produces table 4 in the paper
colnames(TableAppendix) <- c("RR", "$n_{\\text{total}}$",
                             "$p_0 = 0$",
                             "$p_0 = 0.01$",
                             "$p_0 = 0.02$",
                             "$p_0 = 0.05$",
                             "$p_0 = 0.1$",
                             "$p_0 = 0.2$"
                             )
# to fit table on page: use abridged version for table, omit all lines
# where power is > 0.95 for all p_0's
TableAppendix <- TableAppendix[(TableAppendix[, 3] <= 0.95 |
                                  TableAppendix[, 4] <= 0.95 |
                                  TableAppendix[, 5] <= 0.95 |
                                  TableAppendix[, 6] <= 0.95 |
                                  TableAppendix[, 7] <= 0.95 |
                                  TableAppendix[, 8] <= 0.95), ]
print(xtable(TableAppendix,caption = 'Computed power as function of relative risk
             (RR) of death in the new treatment group (risk $p_1$) compared to
             the reference group (risk $p_0$), for given $\\alpha =0.025$,
             total sample size $n_{\\text{total}}$, Sample size allocation for
             reference to new treatment 1:2. Untied case.',
             label = "tab:AppendixUntied",digits = c(0, 0, 0, 3, 3, 3, 3, 3, 3)),
      include.rownames = FALSE,
      caption.placement = "top",
      sanitize.colnames.function = function(x){x})

@
\subsection*{A.2.\enspace Tied case}
In table \ref{tab:AppendixTied} an abridged list of values for power in the tied
case is shown. Again, total sample size is given in steps of 30; only sample
sizes for which at least one observed power is less than or equal to 0.95 are
displayed.

<<xtableD, echo=FALSE, results="asis">>=
# This chunk produces table 4 in the paper
colnames(TableAppendix.Tied) <- c("RR", "$n_{\\text{total}}$",
                             "$p_0 = 0$",
                             "$p_0 = 0.01$",
                             "$p_0 = 0.02$",
                             "$p_0 = 0.05$",
                             "$p_0 = 0.1$",
                             "$p_0 = 0.2$"
                             )
# to fit table on page: use abridged version for table, omit all lines
# where power is > 0.95 for all p_0's
TableAppendix.Tied <- TableAppendix.Tied[(TableAppendix.Tied[, 3] <= 0.95 |
                                  TableAppendix.Tied[, 4] <= 0.95 |
                                  TableAppendix.Tied[, 5] <= 0.95 |
                                  TableAppendix.Tied[, 6] <= 0.95 |
                                  TableAppendix.Tied[, 7] <= 0.95 |
                                  TableAppendix.Tied[, 8] <= 0.95), ]
print(xtable(TableAppendix.Tied,caption = 'Computed power as function of relative risk
             (RR) of death in the new treatment group (risk $p_1$) compared to
             the reference group (risk $p_0$), for given $\\alpha =0.025$,
             total sample size $n_{\\text{total}}$, Sample size allocation for
             reference to new treatment 1:2. Tied case.',
             label = "tab:AppendixTied",digits = c(0, 0, 0, 3, 3, 3, 3, 3, 3)),
      include.rownames = FALSE,
      caption.placement = "top",
      sanitize.colnames.function = function(x){x})

@
\subsection*{A.3.\enspace Simulation results}
In table \ref{tab:simresult} the results of the simulation are shown.

<<xtableB, echo=FALSE, results="asis">>=
# This chunk produces table 4 in the paper
colnames(power_results) <- c("RR", "$p_0$",
                                 "$n$",
                                 "untied: $\\beta = 0.8$",
                                 "$\\beta = 1.0$",
                                 "$\\beta = 1.2$",
                                 "tied: $\\beta = 0.8$",
                                 "$\\beta = 1.0$",
                                 "$\\beta = 1.2$"
                                 )
print(xtable(power_results,caption = 'Observed power as function of relative risk
             (RR) of death under the null hypothesis in the new treatment group
             (risk $p_1$) compared to the reference group (risk $p_0$) and total
             sample size $n$.
             Significance level $\\alpha =0.025$, tied and untied case,
             shape parameter $\\beta$ of the log-logistic time to event
             distribution. Sample size allocation for reference to new treatment 1:2. ',
             label = "tab:simresult",digits = c(0, 0, 3, 0, 3, 3, 3, 3, 3, 3)),
      include.rownames = FALSE,
      caption.placement = "top",
      sanitize.colnames.function = function(x){x})
@

\begin{thebibliography}{10}

\bibitem[Lachin(1999)Lachin, J. L]{bib1} Lachin, J. L. (1999) Worst-rank score
analysis with informatively missing observations in clinical trials.
\textit{Controlled Clinical Trials} \textbf{20}, 408--422.

\bibitem[Wittes et al(1989)Wittes, J., Lakatos, E., and Probstfield, J.]{bib2}
Wittes, J., Lakatos, E., and Probstfield, J. (1989) Surrogate endpoints in
clinical trials: cardiovascular diseases. \textit{Statistics in Medicine}
\textbf{8}, 415--425.

\bibitem[O'Brien(1984) O'Brien, P. C.]{bib3} O'Brien, P. C. (1984) Procedures
for comparing samples with multiple endpoints. \textit{Biometrics}
\textbf{40}, 1079--1087.

\bibitem[Felker et al(2008) Felker, G. M., Anstrom, K. J., Rogers, J. G.]{bib4}
Felker, G. M., Anstrom, K. J., Rogers, J. G. A. (2008) global ranking approach
to end points in trials of mechanical circulatory support devices.
\textit{Journal of Cardiac Failure} \textbf{14}, 368--372.

\bibitem[Felker and Maisel(2010) Felker, G. M., Maisel, A. S.]{bib5} Felker, G. M.,
Maisel, A. S. (2010) A Global Rank End Point for Clinical Trials in Acute Heart
Failure. \textit{Circulation: Heart Failure} \textbf{3}, 643--646.

\bibitem[Matsouaka and Betensky(2015) Matsouaka, R. A., Betensky, R. A.]{bib6}
Matsouaka, R. A., Betensky, R. A. (2015) Power and sample size calculations for
the Wilcoxon–-Mann–-Whitney test in the presence of death-censored observations.
\textit{Statistics in Medicine} \textbf{34}, 406--431.

\bibitem[Matsouaka et al (2018) Matsouaka, R. A., Singhal, A. B., Betensky, R. A.]{bib7}
Matsouaka, R. A., Singhal, A. B.Betensky, R. A. (2018) An optimal
Wilcoxon–-Mann–-Whitney test of mortality and a continuous outcome.
\textit{Statistical Methods in Medical Research} 2018 \textbf{27(8)}, 2384–-2400

\bibitem[Munzel (2009) Munzel, U.]{bib8}
Munzel, U. (2009) Nonparametric non‐inferiority analyses in the three--arm design
with active control and placebos.
\textit{Statistics in Medicine} \textbf{28}, 3643--3656.

\bibitem[Munzel and Hauschke(2003) Munzel, U., Hauschke, D.]{bib9}
Munzel, U., Hauschke, D. (2003) Nonparametric test for proving noninferiority in clinical trials with ordered categorical data.
\textit{Pharmaceutical Statistics} \textbf{2}, 31--37.

\bibitem[Wellek (2010) Wellek, S.]{bib10}
Wellek, S. (2010) \textit{Testing Statistical Hypotheses of Equivalence and
Noninferiority, Second Edition}.
Chapman and Hall / CRC, Boca Raton.

\bibitem[Zhang et al(2015) Zhang, F., Miyaoka, E., Huang, F.,  Tanaka, Y.]{bib11}
Zhang, F., Miyaoka, E., Huang, F.,  Tanaka, Y. (2015)  Test Statistics and
Confidence Intervals to Establish Noninferiority between Treatments with Ordinal
Categorical Data. \textit{Journal of biopharmaceutical statistics} \textbf{5}, 921--938.


\end{thebibliography}
%\newpage
\phantom{aaaa}
\end{document}